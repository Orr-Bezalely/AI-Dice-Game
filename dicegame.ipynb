{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Dice Game\n",
    "## Assignment Preamble\n",
    "Please ensure you carefully read all of the details and instructions on the assignment page, this section, and the rest of the notebook. If anything is unclear at any time please post on the forum or ask a tutor well in advance of the assignment deadline.\n",
    "\n",
    "In addition to all of the instructions in the body of the assignment below, you must also follow the following technical instructions for all assignments in this unit. *Failure to do so may result in a grade of zero.*\n",
    "* [At the bottom of the page](#Submission-Test) is some code which checks you meet the submission requirements. You **must** ensure that this runs correctly before submission.\n",
    "* Do not modify or delete any of the cells that are marked as test cells, even if they appear to be empty.\n",
    "* Do not duplicate any cells in the notebook – this can break the marking script. Instead, insert a new cell (e.g. from the menu) and copy across any contents as necessary.\n",
    "\n",
    "Remember to save and backup your work regularly, and double-check you are submitting the correct version.\n",
    "\n",
    "This notebook is the primary reference for your submission. You may write code in separate `.py` files but it must be clearly imported into the notebook so that it runs without needing to reference those files, and you must explain clearly what functionality is contained in those files (through comments, markdown cells, etc).\n",
    "\n",
    "As always, **the work you submit for this assignment must be entirely your own.** Do not copy or work with other students. Do not copy answers that you find online. \n",
    "\n",
    "\n",
    "## Getting Started\n",
    "For this assignment, you will be writing an agent that can play a simple dice game. Here are the basic rules:\n",
    "* You start with 0 points\n",
    "* Roll three fair six-sided dice\n",
    "* Now choose one of the following:\n",
    " * Stick, accept the values shown. If two or more dice show the same values, then all of them are flipped upside down: 1 becomes 6, 2 becomes 5, 3 becomes 4, and vice versa. The total is then added to your points and this is your final score.\n",
    " * OR reroll the dice. You may choose to hold any combination of the dice on the current value shown. Rerolling costs you 1 point – so during the game and perhaps even at the end your score may be negative. You then make this same choice again.\n",
    "\n",
    "The best possible score for this game is 18 and is achieved by rolling three 1s on the first roll.\n",
    "\n",
    "The reroll penalty prevents you from rolling forever to get this score. If the value of the current dice is greater than the expected value of rerolling them (accounting for the penalty), then you should stick.\n",
    "\n",
    "The optimal decision is independent of your current score. It does not matter whether it is your first roll with a current score of 0, or your twentieth roll with a current score of -19 (in which case a positive end score is impossible), in either of these cases if you roll three 6s (which, if you stick, will only add 3 points) then you still expect to get a *better* end score by rerolling and taking the penalty. Almost any other roll will beat it, so it's still the right choice to maximise your score.\n",
    "\n",
    "It is pretty obvious that you should stick on three 1s, and reroll on three 6s. Should you hold any of the 6s when you reroll? What about other values? What should you do if the dice come up 3, 4, 5?\n",
    "\n",
    "We do not know what numbers will come up when we roll, but we do know exactly what the probability of any given roll is. This is the point of the probabilistic reasoning section of the unit; if we can model the true probabilities then we can mathematically calculate the optimal policy. Not all real world situations use dice, but these techniques work well even if we can only estimate the true probabilities.\n",
    "\n",
    "### Play The Game\n",
    "You can play the game in the following cell. Change the `SKIP_GAME` constant to `False` to enable this cell. \n",
    "<br> *Make sure to change it back to `True` before submitting.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's play the game!\n",
      "Your dice are (1, 2, 4)\n",
      "Your score is 0\n",
      "Type which dice you want to hold separated by spaces indexed from 0, blank to reroll all\n",
      "Hold all dice to stick and get your final score\n",
      ">0\n",
      "Your dice are (1, 2, 4)\n",
      "Your score is -1\n",
      "Type which dice you want to hold separated by spaces indexed from 0, blank to reroll all\n",
      "Hold all dice to stick and get your final score\n",
      ">0\n",
      "Your dice are (1, 3, 5)\n",
      "Your score is -2\n",
      "Type which dice you want to hold separated by spaces indexed from 0, blank to reroll all\n",
      "Hold all dice to stick and get your final score\n",
      ">0\n",
      "Your dice are (1, 3, 4)\n",
      "Your score is -3\n",
      "Type which dice you want to hold separated by spaces indexed from 0, blank to reroll all\n",
      "Hold all dice to stick and get your final score\n",
      ">0\n",
      "Your dice are (1, 3, 3)\n",
      "Your score is -4\n",
      "Type which dice you want to hold separated by spaces indexed from 0, blank to reroll all\n",
      "Hold all dice to stick and get your final score\n",
      ">0 1 2\n",
      "Your final dice are (1, 4, 4)\n",
      "Your final score is 5\n",
      "Play again? y/n\n",
      ">y\n",
      "Your dice are (2, 3, 3)\n",
      "Your score is 0\n",
      "Type which dice you want to hold separated by spaces indexed from 0, blank to reroll all\n",
      "Hold all dice to stick and get your final score\n",
      ">0 1 2\n",
      "Your final dice are (2, 4, 4)\n",
      "Your final score is 10\n",
      "Play again? y/n\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\Desktop\\BATH\\FIRST YEAR\\semester 2\\Artificial Intelligence\\dicegame\\dicegame\\dicegame\\dice_game.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    665\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    666\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"__main__\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 667\u001b[1;33m     \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    668\u001b[0m     \u001b[1;31m# import time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    669\u001b[0m     \u001b[1;31m#\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\BATH\\FIRST YEAR\\semester 2\\Artificial Intelligence\\dicegame\\dicegame\\dicegame\\dice_game.py\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    659\u001b[0m                 \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    660\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Play again? y/n\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 661\u001b[1;33m         \u001b[0magain\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\">\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    662\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0magain\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m\"y\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    663\u001b[0m             \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[1;34m(self, prompt)\u001b[0m\n\u001b[0;32m    858\u001b[0m                 \u001b[1;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    859\u001b[0m             )\n\u001b[1;32m--> 860\u001b[1;33m         return self._input_request(str(prompt),\n\u001b[0m\u001b[0;32m    861\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    862\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[1;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[0;32m    902\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    903\u001b[0m                 \u001b[1;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 904\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Interrupted by user\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    905\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    906\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Invalid Message:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">n\n"
     ]
    }
   ],
   "source": [
    "SKIP_GAME = False\n",
    "if not SKIP_GAME:\n",
    "    %run dice_game.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submission Requirements\n",
    "The code supports playing this game with many possible modifications – you can change the number of dice, the values on the dice, or even make biased (weighted) dice that are more likely to roll certain values. More on this later.\n",
    "\n",
    "For this assignment you will need to submit:\n",
    "1. The implementation for an agent which can play the game – this notebook\n",
    " * You can write this however you like, using the unit material or otherwise\n",
    " * Your code will be subject to automated testing, from which grades will be assigned depending on how well your agent plays the game (potentially with modifications)\n",
    " * To get a high grade on this assignment, the speed of your code will also be a factor – the quicker the better\n",
    " * There are some sample tests and skeleton code included below, make sure your code is compatible with the format of these tests\n",
    "2. A text file that explains your approach and the decisions you made in your own words – a readme file\n",
    " * Submissions that do not include the written section will receive zero marks – **this part is mandatory**\n",
    " * You may write your file in plain text (.txt) or [Markdown](https://www.markdownguide.org/basic-syntax/) (.md)\n",
    " * To get top marks on this assignment, as well as getting a high grade from your implementation, you must also demonstrate excellent academic presentation in your written section\n",
    " \n",
    "### Choice of Algorithm\n",
    "You can take any approach you like to write your agent. We have provided two examples for you, though these agents do not perform very well, and would not pass the assignment, they should help you structure your code.\n",
    "\n",
    "We have covered *value iteration* in the unit, and this technique will work well here to produce a strategy for the game, which your agent can then follow. It is up to you to work out the parameters that will be suit the value iteration algorithm to maximise your score.\n",
    "\n",
    "However, there are many other possible options. Simply calculating the expected value of a single roll will produce a much stronger strategy than playing randomly. You could also look up various other approaches that can be applied to Markov decision processes, such as policy iteration.\n",
    "\n",
    "To get a high grade on this assignment will require a particularly efficient implementation value iteration with intelligent choice of parameters, or something which goes beyond the material we have presented. *This is left unguided and is not factored into the unit workload estimates.*\n",
    "\n",
    "Note that you can write your agent to support modified versions of the game, which is also demonstrated below. For submissions that support this, the tests will include modified versions of the game, some where the game is *more obvious*, and some where it is *less obvious* than the default rules. Getting high marks requires supporting this feature. For those who are struggling to write a good agent, supporting this feature will result in additional tests which are more forgiving, so this might be an attractive option.\n",
    "\n",
    "If you choose to implement more than one algorithm, please feel free to include your code and write about it in part two (readme file), but only the code in this notebook will be used in the automated testing.\n",
    "\n",
    "## Dice Game Class\n",
    "A class called `DiceGame` is provided within `dice_game.py` for you to use in your solution. Here is a demonstration of its features.\n",
    "\n",
    "When you create a DiceGame object, by default you will get the rules as stated above. \n",
    "```python\n",
    "game = DiceGame()\n",
    "```\n",
    "Creates a game with 3 normal 6-sided dice. \n",
    "\n",
    "You may wish to modify the game mechanics, and you can do this using the other constructor arguments, for example:\n",
    "```python\n",
    "game = DiceGame(dice=4, sides=3, values=[1, 2, 6], bias=[0.1, 0.1, 0.8], penalty=2)\n",
    "```\n",
    "will create a game where you roll 4 dice, each with 3 sides, labelled 1, 2, and 6, where each die is far more likely to roll a 6 than they are to roll a 1 or a 2, and furthermore the penalty for rerolling is now 2 points instead of 1. *Note: this does not necessarily result in an interesting game.*\n",
    "\n",
    "In games with unusual values or sides (3-sided dice are unusual without trying to turn them upside down), when there are duplicates, `value[i]` becomes `value[-i]`. With odd-sided dice, the middle value will flip onto itself.\n",
    "\n",
    "Once created, the `DiceGame` object can be run in two different modes, *simulation* and *analysis*. It is likely that you will mostly use [*analysis* mode](#Analysis-Mode) to derive your agent's behaviour, but either way some understanding of simulation mode might be useful.\n",
    "\n",
    "### Simulation Mode\n",
    "The object provides the methods required to simulate playing the game. This might be useful if you want to test your agent, or you just want to try playing the game yourself, as we did in the cell earlier. The current dice values are found by calling `get_dice_state()`, they will always be listed in ascending order. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dice_game import DiceGame\n",
    "import numpy as np\n",
    "\n",
    "# setting a seed for the random number generator gives repeatable results, making testing easier!\n",
    "np.random.seed(111)\n",
    "\n",
    "game = DiceGame()\n",
    "game.get_dice_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To roll the dice, you call the `roll` method which takes one parameter: a tuple representing which dice you want to hold, numbered from zero. We rolled (2, 3, 4). Suppose we want to hold the 2, we would pass the tuple `(0,)` into the `roll` method (note we need to include the comma so that Python knows this is a tuple).\n",
    "\n",
    "The `roll` method returns a tuple containing: the reward for this action, the new state, and whether the game is over."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward, new_state, game_over = game.roll((0,))\n",
    "print(reward)\n",
    "print(new_state)\n",
    "print(game_over)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now suppose we are happy and wish to stick to get our final score. We can call the `roll` method and supply a tuple containing all three dice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward, new_state, game_over = game.roll((0, 1, 2))\n",
    "print(reward)\n",
    "print(new_state)\n",
    "print(game_over)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that the return value is just the reward for the action, in this case 15. To get our final score we can inspect `game.score`. We rerolled once, so expect to get a score of 14."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(game.score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And to play again we can call `game.reset()` which returns the new starting dice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis Mode\n",
    "In analysis mode, you are not playing the game, but asking the object for *all possible outcomes of certain actions*.\n",
    "\n",
    "First of all, it is useful to know that all the possible states and all the possible actions are stored inside the object. Try changing the game mechanics on the first line (e.g. add `dice=2` to the constructor) and see how the other information is updated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game = DiceGame()\n",
    "print(f\"The first 5 of {len(game.states)} possible dice rolls are: {game.states[0:5]}\")\n",
    "print(f\"The possible actions on any given turn are: {game.actions}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the most important method is `get_next_states(action, dice_state)`. This allows you to get all the possible resulting states for any given state and action.\n",
    "\n",
    "Earlier we had the roll of `(2, 3, 4)` and decided to hold the 2. The game can calculate all possible outcomes for us, and crucially will also give us the probability of each state occurring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game = DiceGame()\n",
    "states, game_over, reward, probabilities = game.get_next_states((0,), (2, 3, 4))\n",
    "for state, probability in zip(states, probabilities):\n",
    "    print(f\"Would get roll of {state} with probability {probability}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method also works consistently when all dice are held, reporting that this action would cause the game to be over and giving the reward. Note that the list of states returned contains the value `None`. This is to denote that the game has entered a terminal state – no further actions would be allowed. The game does not return the final dice here, because that is another valid state (from which there would still be actions available). Also, the `reward` value is not the same as the final `score` of any given game, because it does not include any possible previous penalties for rerolling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states, game_over, reward, probabilities = game.get_next_states((0, 1, 2), (2, 2, 5))\n",
    "print(states)\n",
    "print(game_over)\n",
    "print(reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part One\n",
    "You should write all of your code for your dice game agent below this cell.\n",
    "\n",
    "Let's start with some example agents, so you can see the format we will use. In the cell below are two agents which do not play particularly well. One always holds immediately, the other will keep re-rolling all dice until they get the best possible dice (`(1, 1, 1)` or `(1, 1, 6)`), ignoring the massive penalty this will incur from re-rolling. Neither of them is considering the probabilities involved in the game.\n",
    "\n",
    "There is also a function which will run the game with an instance of a given agent. When you run the cell, it will simulate a game with each agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from dice_game import DiceGame\n",
    "import numpy as np\n",
    "\n",
    "class DiceGameAgent(ABC):\n",
    "    def __init__(self, game):\n",
    "        self.game = game\n",
    "    \n",
    "    @abstractmethod\n",
    "    def play(self, state):\n",
    "        pass\n",
    "\n",
    "\n",
    "class AlwaysHoldAgent(DiceGameAgent):\n",
    "    def play(self, state):\n",
    "        return (0, 1, 2)\n",
    "\n",
    "\n",
    "class PerfectionistAgent(DiceGameAgent):\n",
    "    def play(self, state):\n",
    "        if state == (1, 1, 1) or state == (1, 1, 6):\n",
    "            return (0, 1, 2)\n",
    "        else:\n",
    "            return ()\n",
    "        \n",
    "        \n",
    "def play_game_with_agent(agent, game, verbose=False):\n",
    "    state = game.reset()\n",
    "    \n",
    "    #if(verbose): print(f\"Testing agent: \\n\\t{type(agent).__name__}\")\n",
    "    #if(verbose): print(f\"Starting dice: \\n\\t{state}\\n\")\n",
    "    \n",
    "    game_over = False\n",
    "    actions = 0\n",
    "    while not game_over:\n",
    "        action = agent.play(state)\n",
    "        actions += 1\n",
    "        \n",
    "        #if(verbose): print(f\"Action {actions}: \\t{action}\")\n",
    "        _, state, game_over = game.roll(action)\n",
    "        #if(verbose and not game_over): print(f\"Dice: \\t\\t{state}\")\n",
    "\n",
    "    #if(verbose): print(f\"\\nFinal dice: {state}, score: {game.score}\")\n",
    "        \n",
    "    return game.score\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # random seed makes the results deterministic\n",
    "    # change the number to see different results\n",
    "    # or delete the line to make it change each time it is run\n",
    "    np.random.seed(1)\n",
    "    \n",
    "    game = DiceGame()\n",
    "    \n",
    "    agent1 = AlwaysHoldAgent(game)\n",
    "    play_game_with_agent(agent1, game, verbose=True)\n",
    "    \n",
    "    print(\"\\n\")\n",
    "    \n",
    "    agent2 = PerfectionistAgent(game)\n",
    "    play_game_with_agent(agent2, game, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below, write the logic for your own agent. To do this, you need to write a class named `MyAgent` which is a subclass of `DiceGameAgent`, which overrides the `play(self, state)` method. There is some skeleton code in the cell below to help you get started. This is the code which will be tested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c23c3b5f5b787d247eba0e6f24fcf348",
     "grade": false,
     "grade_id": "cell-cb208d0e0467d373",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class MyAgent(DiceGameAgent):\n",
    "    def __init__(self, game):\n",
    "        \"\"\"\n",
    "        if your code does any pre-processing on the game, you can do it here\n",
    "\n",
    "        e.g. you could do the value iteration algorithm here once, store the policy,\n",
    "        and then use it in the play method\n",
    "\n",
    "        you can always access the game with self.game\n",
    "        \"\"\"\n",
    "        # this calls the superclass constructor (does self.game = game)\n",
    "        super().__init__(game)\n",
    "        # Initialising the state_dict to initial value of 0 for each state,\n",
    "        # and the best action to None in the policy dictionary\n",
    "        self.state_value, self.policy = {state: 0 for state in game.states}, {state: None for state in game.states}\n",
    "        # Initialises a dictionary of dictionaries which stores the transition model, whether the game is over and reward for each (state,action)\n",
    "        self.transition_dict = {}\n",
    "        for state in game.states:\n",
    "            action_dict = {}\n",
    "            for action in game.actions:\n",
    "                states, game_over, reward, probabilities = game.get_next_states(action, state)\n",
    "                action_dict[action] = list(zip(states, probabilities)), game_over, reward\n",
    "            self.transition_dict[state] = action_dict\n",
    "        # Initialise the values of gamma (value between 0 and 1 representing discount rate)\n",
    "        # and theta (representing the value below which the policy is considered to have converged)\n",
    "        self.gamma, self.theta = 1, 0.001\n",
    "        self.initialise_policy()\n",
    "\n",
    "    def bellman_equation(self, transition_model, reward):\n",
    "        \"\"\"\n",
    "        Computes the value of a state by considering all the successor states and their likelihoods.\n",
    "        :param transition_model: list of tuples (state,probability) representing the transition model\n",
    "        :param reward: integer representing the reward gained for the action taken\n",
    "        :return: float representing the expected value of the given state following the policy (i.e. given an action)\n",
    "        \"\"\"\n",
    "        expected_action_value = 0\n",
    "        for next_state, probability in transition_model:\n",
    "            expected_action_value += probability * (reward + (self.gamma * self.state_value[next_state]))\n",
    "        return expected_action_value\n",
    "\n",
    "    def action_value_function(self, action, state):\n",
    "        \"\"\"\n",
    "        Calculates the expected value the 'state' will yield by doing 'action'\n",
    "        :param action: tuple representing the action taken (i.e. which dice to hold)\n",
    "        :param state: tuple representing the number rolled on each die\n",
    "        :return: float representing the expected value by doing action on state\n",
    "        \"\"\"\n",
    "        # Gets the transition model, whether the game is over and the reward of taking the action 'action' from state 'state'.\n",
    "        transition_model, game_over, reward = self.transition_dict[state][action]\n",
    "        # Checks if the game is over, and if so then it returns the reward\n",
    "        if game_over:\n",
    "            return reward\n",
    "        # Returns the expected value of the given state following the policy (i.e. given an action)\n",
    "        return self.bellman_equation(transition_model, reward)\n",
    "\n",
    "    def best_action_and_val(self, state):\n",
    "        \"\"\"\n",
    "        Calculates the optimal action and its value and returns them\n",
    "        :param state: tuple representing the number rolled on each die\n",
    "        :return: tuple representing the best action (i.e. which dice to hold) and a float representing its value\n",
    "        \"\"\"\n",
    "        # Sets the max_action_val, best_action to default values which will get overwritten in the first iteration\n",
    "        best_action, max_action_val = None, float(\"-inf\")\n",
    "        # Loops through every action, and gets its action value, updating max_action_val and best_action\n",
    "        # if it has a higher predicted value.\n",
    "        for action in game.actions:\n",
    "            current_action_value = self.action_value_function(action, state)\n",
    "            # Updates the best action and its value if the current_action_value is bigger than\n",
    "            # the maximum one recorded until now.\n",
    "            if current_action_value > max_action_val:\n",
    "                best_action, max_action_val = action, current_action_value\n",
    "        return best_action, max_action_val\n",
    "\n",
    "    def initialise_policy(self):\n",
    "        \"\"\"\n",
    "        Calculates the optimal policy using value iteration.\n",
    "        :return: Dictionary representing the optimal policy with keys representing the possible states\n",
    "        and values representing the action that should be taken in the given state.\n",
    "        \"\"\"\n",
    "        # Initialising the current_iter_dict to initial value of 0 for each state.\n",
    "        # this dictionary exists as we need to take the values from the previous iteration and not the current iteration\n",
    "        current_iter_dict = {state: 0 for state in game.states}\n",
    "        while True:\n",
    "            # Sets delta (representing the current iteration's biggest change value) to 0\n",
    "            delta = 0\n",
    "            # Iterates through each state and gets the best action and its predicted value based on previous iterations\n",
    "            for cur_state in self.state_value:\n",
    "                # Gets the best action and its predicted value based on previous iterations\n",
    "                best_action, max_action_val = self.best_action_and_val(cur_state)\n",
    "                # Updates delta throughout each state\n",
    "                delta = max(delta, abs(self.state_value[cur_state] - max_action_val))\n",
    "                # Updates the policy and the current_iter_dict dictionaries with the values calculated above\n",
    "                self.policy[cur_state], current_iter_dict[cur_state] = best_action, max_action_val\n",
    "            # Updates the states_dict at the end of the iteration\n",
    "            self.state_value = current_iter_dict.copy()\n",
    "            # Checks whether the current iteration's delta value has converged below the theta value set at the beginning,\n",
    "            # if so then the process is stopped\n",
    "            if delta < self.theta:\n",
    "                return\n",
    "\n",
    "    def play(self, state):\n",
    "        \"\"\"\n",
    "        given a state, return the chosen action for this state\n",
    "        at minimum you must support the basic rules: three six-sided fair dice\n",
    "\n",
    "        if you want to support more rules, use the values inside self.game, e.g.\n",
    "            the input state will be one of self.game.states\n",
    "            you must return one of self.game.actions\n",
    "\n",
    "        read the code in dicegame.py to learn more\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        return self.policy[state]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You're free to modify the structure of the code above as you like, but you must make sure your code is compatible with our testing framework, which you can check below.\n",
    "\n",
    "All of your code must go above this cell. You may add additional cells into the notebook if you wish, but do not duplicate or copy/paste cells as this can interfere with the grading script.\n",
    "\n",
    "### Testing Details\n",
    "Your agent will be tested in two ways. First it will be tested in actual random games, and the average score will be measured. All students will get the exact same dice rolls, so the best strategies will get the most points. Second, it will be analysed in specific circumstances (i.e. specific dice rolls) to test what it does compared to the optimal strategy.\n",
    "\n",
    "In addition, if your code supports it, we will repeat these tests with *extended rules*, i.e. not using the default game with three 6-sided fair dice. You can read more about how this affects grading in [this section](#Choice-of-Algorithm). \n",
    "\n",
    "The tests run with a timeout to prevent any submissions slowing down the entire grading. On the testing machine, your agent must take less than 30 seconds to construct, and less than 2 seconds to produce each action. For high marks, execution time will also factor into grading (both construction and average per move time).\n",
    "\n",
    "As we have said before, the best way to improve the performance of your code is through a detailed understanding and smart choice of AI algorithms. This assignment is ***not*** meant to test your ability to write multi-threaded code or any other kind of high-performance code optimisations. \n",
    "\n",
    "#### Test Cell\n",
    "Run the following cell to test your code 10 times and print the average score. You should run this test to ensure your code is working correctly. The actual tests which determine your grade will be different and more thorough. \n",
    "\n",
    "To enable the tests, set the constant `SKIP_TESTS` to `False`.\n",
    "\n",
    "**IMPORTANT**: you must set `SKIP_TESTS` back to `True` before submitting this file!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing basic rules.\n",
      "\n",
      "\n",
      "Average score: 13.3919\n",
      "Total time: 4.3281 seconds\n"
     ]
    }
   ],
   "source": [
    "SKIP_TESTS = False\n",
    "\n",
    "if not SKIP_TESTS:\n",
    "    import time\n",
    "\n",
    "    total_score = 0\n",
    "    total_time = 0\n",
    "    n = 10000\n",
    "\n",
    "    np.random.seed(1)\n",
    "\n",
    "    print(\"Testing basic rules.\")\n",
    "    print()\n",
    "\n",
    "    game = DiceGame()\n",
    "\n",
    "    start_time = time.process_time()\n",
    "    test_agent = MyAgent(game)\n",
    "    total_time += time.process_time() - start_time\n",
    "\n",
    "    for i in range(n):\n",
    "        start_time = time.process_time()\n",
    "        score = play_game_with_agent(test_agent, game)\n",
    "        total_time += time.process_time() - start_time\n",
    "\n",
    "        #print(f\"Game {i} score: {score}\")\n",
    "        total_score += score\n",
    "\n",
    "    print()\n",
    "    print(f\"Average score: {total_score/n}\")\n",
    "    print(f\"Total time: {total_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Opt-In For Extended Rules\n",
    "If you wish your code to be tested with rules other than the defaults, set the constant `TEST_EXTENDED_RULES` to `True` on the next line. Another test will be performed to check your code still works. If you get an error, you are likely not supporting the extended rules properly.\n",
    "\n",
    "Refer to [this section](#Choice-of-Algorithm) to understand more about how extended rules factor into your possible grade.\n",
    "\n",
    "**Note:** you need to have `SKIP_TESTS` set to `False` in the cell above (and run it!) to enable the tests below. The value of `TEST_EXTENDED_RULES` *will* be used in the grading tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing extended rules – two three-sided dice.\n",
      "\n",
      "\n",
      "Average score: 16.5554\n",
      "Average time: 0.00067 seconds\n"
     ]
    }
   ],
   "source": [
    "TEST_EXTENDED_RULES = True\n",
    "\n",
    "if not SKIP_TESTS and TEST_EXTENDED_RULES:\n",
    "    total_score = 0\n",
    "    total_time = 0\n",
    "    n = 10000\n",
    "\n",
    "    print(\"Testing extended rules – two three-sided dice.\")\n",
    "    print()\n",
    "\n",
    "    game = DiceGame(dice=4, sides=6, penalty = 2)\n",
    "\n",
    "    start_time = time.process_time()\n",
    "    test_agent = MyAgent(game)\n",
    "    total_time += time.process_time() - start_time\n",
    "\n",
    "    for i in range(n):\n",
    "        start_time = time.process_time()\n",
    "        score = play_game_with_agent(test_agent, game)\n",
    "        total_time += time.process_time() - start_time\n",
    "\n",
    "        #print(f\"Game {i} score: {score}\")\n",
    "        total_score += score\n",
    "\n",
    "    print()\n",
    "    print(f\"Average score: {total_score/n}\")\n",
    "    print(f\"Average time: {total_time/n:.5f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission Test\n",
    "The following cell tests if your notebook is ready for submission. **You must not skip this step!**\n",
    "\n",
    "Restart the kernel and run the entire notebook (Kernel → Restart & Run All). Now look at the output of the cell below. \n",
    "\n",
    "*If there is no output, then your submission is not ready.* Either your code is still running (did you forget to skip tests?) or it caused an error.\n",
    "\n",
    "As previously mentioned, failing to follow these instructions can result in a grade of zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5279267fc523c59f92499fbc338b9a65",
     "grade": false,
     "grade_id": "cell-0e662447caaa3bcc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import pathlib\n",
    "\n",
    "fail = False;\n",
    "\n",
    "if not SKIP_GAME:\n",
    "    fail = True;\n",
    "    print(\"You must set the SKIP_GAME constant to True in the earlier cell.\")\n",
    "\n",
    "if not SKIP_TESTS:\n",
    "    fail = True;\n",
    "    print(\"You must set the SKIP_TESTS constant to True in the earlier cell.\")\n",
    "    \n",
    "p1 = pathlib.Path('./readme.txt')\n",
    "p2 = pathlib.Path('./readme.md')\n",
    "if not (p1.is_file() or p2.is_file()):\n",
    "    fail = True;\n",
    "    print(\"You must include a separate file called readme.txt or readme.md in your submission.\")\n",
    "    \n",
    "p3 = pathlib.Path('./dicegame.ipynb')\n",
    "if not p3.is_file():\n",
    "    fail = True\n",
    "    print(\"This notebook file must be named dicegame.ipynb\")\n",
    "    \n",
    "if \"MyAgent\" not in dir():\n",
    "    fail = True;\n",
    "    print(\"You must include a class called MyAgent as defined above.\")\n",
    "else:    \n",
    "    game = DiceGame()\n",
    "    agent = MyAgent(game)\n",
    "    action = agent.play((1, 1, 1))\n",
    "\n",
    "    if action not in game.actions:\n",
    "        print(\"Warning:\")\n",
    "        print(\"Your agent does not seem to produce a valid action with the default rules.\")\n",
    "        print()\n",
    "        print(\"Your assignment is unlikely to get any marks from the autograder. While we will\")\n",
    "        print(\"try to check it manually to assign some partial credit, we encourage you to ask\")\n",
    "        print(\"for help on the forum or directly to a tutor.\")\n",
    "        print()\n",
    "        print(\"Please use the readme file to explain your code anyway.\")\n",
    "        \n",
    "    if TEST_EXTENDED_RULES:\n",
    "        print(\"Info: TEST_EXTENDED_RULES is ON\")\n",
    "        game = DiceGame(dice=2, sides=8)\n",
    "        agent = MyAgent(game)\n",
    "        try:\n",
    "            action = agent.play((7, 8))\n",
    "        except:\n",
    "            action = None\n",
    "\n",
    "        if action not in game.actions:\n",
    "            fail = True\n",
    "            print(\"Your agent does not produce a valid action with the extended rules.\")\n",
    "            print(\"Turn off TEST_EXTENDED_RULES if you cannot fix this error.\")\n",
    "    else:\n",
    "        print(\"Info: TEST_EXTENDED_RULES is OFF (extended rules will not be tested)\")\n",
    "    \n",
    "if fail:\n",
    "    print()\n",
    "    sys.stderr.write(\"Your submission is not ready! Please read and follow the instructions above.\")\n",
    "else:\n",
    "    print()\n",
    "    print(\"All checks passed. When you are ready to submit, upload the notebook and readme file to the\")\n",
    "    print(\"assignment page, without changing any filenames.\")\n",
    "    print()\n",
    "    print(\"If you need to submit multiple files, you can archive them in a .zip file. (No other format.)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2e79088eef80312d3d223f6a463e5648",
     "grade": true,
     "grade_id": "cell-7ffe19fac82d0fb1",
     "locked": true,
     "points": 100,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This is a TEST CELL. Do not delete or change."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
